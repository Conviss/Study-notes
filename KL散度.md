# KL散度

Kullback-Leibler散度用于比较两个概率分布，通常简称为KL散度。通常在概率和统计中，我们会用更简单的近似分布来代替观察到的数据或复杂的分布。KL散度帮助我们衡量在选择近似值时损失了多少信息

### 信息熵

KL散度起源于信息论。信息论的主要目标是量化数据中有多少信息。信息论中最重要的指标称为熵，通常表示为H。概率分布的熵的定义是:
$$
H = - \sum_{i=1}^{N}p(x_{i})logp(x_{i})
$$
其中 $p(x_{i})$ 代表随机事件 x 为 $x_{i}$ 的概率

信息量度量的是一个具体事件发生了所带来的信息，而熵则是在结果出来之前对可能产生的信息量的期望——考虑该随机变量的所有可能取值，即所有可能发生事件所带来的信息量的期望。



信息的大小跟随机事件的概率有关。越小概率的事情发生了产生的信息量越大。越大概率的事情发生了产生的信息量越小。

由 $y = -log(x)$ 得知，当 x 越接近于1，y 就越接近于0。且负号是为了使信息为正数。

如果我们有俩个不相关的事件x和y，那么我们观察到的俩个事件同时发生时获得的信息应该等于观察到的事件各自发生时获得的信息之和，即：

$ h(x,y) = h(x) + h(y)$ 由于x，y是俩个不相关的事件，那么满足 $ p(x,y) = p(x)\times p(y)$

根据上面推导，可以看出h(x)一定与p(x)的对数有关，因为只有对数形式的真数相乘之后，能够对应对数的相加形式。

$ logx + logy = log(x \times y)$

### 使用KL散度测量丢失的信息

Kullback-Leibler散度只是对我们的熵公式的略微修改。不仅仅是有我们的概率分布p，还有上近似分布q。然后，我们查看每个log值的差异：
$$
D_{KL}(p||q ) = \sum_{i=1}^{N}p(x_{i})(logp(x_{i})-logq(x_{i}))
$$
本质上，我们用KL散度看的是对原始分布中的数据概率与近似分布之间的对数差的期望。
$$
D_{KL}(p||q ) = E[logp(x_{i})-logq(x_{i})]
$$

查看KL散度的更常见方法如下：

$$
D_{KL}(p||q ) = \sum_{i=1}^{N}p(x_{i})(log\frac{p(x_{i})}{q(x_{i})} )
$$

KL散度不能看作距离，我们不能使用KL散度来测量两个分布之间的距离，因为KL散度不是对称的。
$$
D_{KL}(p||q ) 不一定等于D_{KL}(q||p )
$$

### 使用KL散度进行优化

当选择近似分布来代替观察到的数据或复杂的分布时，可以分别求出每一个近似分布的KL散度，当KL散度数值较大时表示损失的信息较多，当KL散度数值较小时表示损失的信息较少。所以可以选择KL散度值较小的作为近似分布。

对于已选择的近似分布，我们可以用KL散度数值来判断这个近似分布的参数选择。通过不同的参数选择，会有不同KL散度值，其中我们只需要选择KL散度值最小的作为该近似分布的参数。
