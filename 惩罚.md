# 惩罚

对于一个需要拟合的函数 $f(x)$ 如果维度很高，函数很复杂。其拟合的结果可能会是过拟合。

过拟合：过于符合训练集的数据，使得训练集以外的数据预测效果不佳。

为了防止此类问题，我们对函数 $f(x)$ 添加了正则化项，用于惩罚某些参数，使得函数趋于简单，提高泛化性。

常见的正则化项包括L1正则化和L2正则化两种。



设函数 $f(\vec{x}^{(i)}) = w_{1} x_{1} + w_{2} x_{2}^{2} + w_{3} x_{3}^{3} + b$ 

则损失函数为 $J(\vec{w},b) = \frac{1}{2m}\sum_{i=1}^{m}(\hat{y}^{(i)}  - y^{(i)})^{2}$

添加正则化项后：
$$
J(\vec{w},b) = \frac{1}{2m}\sum_{i=1}^{m}(\hat{y}^{(i)}  - y^{(i)})^{2} + \frac{\lambda}{m}\sum_{j}^{n}w_{j}^{2}
$$
这是对每一个参数 $w_{j}$ 的惩罚。

$\lambda$ 表示惩罚系数，是指机器学习算法的正则化项中的一个参数，其作用是控制正则化过程对误差的调整程度。

惩罚系数是正则化项中的一个重要参数，通过调节惩罚系数的大小来控制正则化的程度。具体来说，惩罚系数越大，正则化程度就越高，对误差的调整作用也就越强，这样可以使得模型更加简单，但是可能会降低模型的拟合能力。而惩罚系数越小，则正则化程度会降低，模型的拟合能力会增强，但是可能会导致模型过于复杂从而泛化性能下降。

惩罚系数的取值通常需要根据实际情况进行调参，通常可以通过交叉验证等方法来确定惩罚系数的取值。

例如：$\lambda$ 值很大，计算出的损失函数值也就很大，因此需要加强对 $w_{j}$ 的惩罚，所以在梯度下降时 $w_{j}$ 会降低更低。当 $w_{j}$ 降到接近于0时，那么在函数中该项约等于0。假设 $w_{3}$ 的值此时为0.00000001，那么函数
$$
f(\vec{x}^{(i)}) = w_{1} x_{1} + w_{2} x_{2}^{2} + w_{3} x_{3}^{3} + b \approx f(\vec{x}^{(i)}) = w_{1} x_{1} + w_{2} x_{2}^{2} + b
$$
 这就大大简化了函数
