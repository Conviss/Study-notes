# 梯度下降

梯度下降法主要分为三种，

- 批量梯度下降
- 随机梯度下降
- 小批量梯度下降

例：预测房价于面积之间的线性关系，使用线性回归拟合一条直线用以预测x面积的房子的房价y

设直线方程为 $f\left ( x \right ) = w x + b$ 

则损失函数为 $J(w,b) = \frac{1}{2m}\sum_{i=1}^{m}(\hat{y}^{(i)}  - y^{(i)})^{2}$

我们的目标是最小化损失函数，即 $\min J(w,b)$，其中 $\hat{y}$ 是预测值，$y$ 是真实值，那么需要对参数w，b使用梯度下降法：
$$
w = w - \alpha \frac{\partial J(w,b)}{\partial w} \\\\
b = b - \alpha \frac{\partial J(w,b)}{\partial b} \\\\
\frac{\partial J(w,b)}{\partial w} = \frac{1}{m}\sum_{i = 1}^{m}(\hat{y}^{(i)}  - y^{(i)}) x^{(i)} \\\\
\frac{\partial J(w,b)}{\partial b} = \frac{1}{m}\sum_{i = 1}^{m}(\hat{y}^{(i)}  - y^{(i)})
$$

#### $\alpha$ 表示学习率（步长）：

用来控制算法每次移动的距离。正常来说， $\alpha$ 越大，算法每次移动的就越大， $\alpha$ 越小，每次移动的也就越小。当 $\alpha$ 太小时，可能会需要多次迭代才能收敛，当 $\alpha$ 太大时，可能导致无法收敛或者会发散，只有选择合适的 $\alpha$ 时，才能更好的收敛。

#### $\frac{\partial J(w,b)}{\partial w}$ 表示 $J(w,b)$ 对 $w$ 求偏导数：

用来表示下降的方向。假设此时该 $w$ 求出偏导数大于0，表示 $w$ 在最低点的右边。所以 $w = w - \alpha \frac{\partial J(w,b)}{\partial w}$ 以减少 $w$ 值。若偏导数小于0，表示 $w$ 在最低点的左边，此时对于 $w$ 来说减去一个负数等于加上这个负数的绝对值，所以增加了 $w$ 值。直到 $w$ 值趋近最低点收敛。

![梯度下降](D:\Users\kangkang\Desktop\硕士学习\学习笔记\img\梯度下降\梯度下降.png)

## 批量梯度下降
批量梯度下降使用整个训练数据集来计算梯度

**优点：**

（1）在训练过程中，使用固定的学习率，不必担心学习率衰退现象的出现。

（2）由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，一定能收敛到全局最小值，如果目标函数非凸则收敛到局部最小值。

（3）它对梯度的估计是无偏的。样例越多，标准差越低。

（4）一次迭代是对所有样本进行计算，此时利用向量化进行操作，实现了并行。

**缺点：**

（1）尽管在计算过程中，使用了向量化计算，但是遍历全部样本仍需要大量时间，尤其是当数据集很大时（几百万甚至上亿），就有点力不从心了。

（2）每次的更新都是在遍历全部样例之后发生的，这时才会发现一些例子可能是多余的且对参数更新没有太大的作用。

## 随机梯度下降

如果使用批量梯度下降法那么每次迭代过程中都要对个整个训练数据集进行求梯度，所以开销非常大，随机梯度下降的思想就是每次迭代选择一个样本来更新参数，那么计算开销就从 $O(n)$ 下降到 $O(1)$

**优点：**

（1）在学习过程中加入了噪声，提高了泛化误差。

（2）由于不是在全部训练数据上的损失函数，而是在每轮迭代中，随机优化某一条训练数据上的损失函数，这样每一轮参数的更新速度大大加快。

**缺点：**

（1）不收敛，在最小值附近波动。

（2）不能在一个样本中使用向量化计算，学习过程变得很慢。

（3）单个样本并不能代表全体样本的趋势。

（4）当遇到局部极小值或鞍点时，SGD会卡在梯度为0处。

## 小批量梯度下降
随机梯度下降虽然提高了计算效率，降低了计算开销，但是由于每次迭代只随机选择一个样本，因此随机性比较大，所以下降过程中非常曲折。所以，样本的随机性会带来很多噪声，我们可以选取一定数目的样本组成一个小批量样本，然后用这个小批量更新梯度，这样不仅可以减少计算成本，还可以提高算法稳定性。

**优点：**

（1）计算速度比批量梯度下降快，因为只遍历部分样例就可执行更新。

（2）随机选择样例有利于避免重复多余的样例和对参数更新较少贡献的样例。

（3）每次使用一个批量可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果。

**缺点：**

（1）在迭代的过程中，因为噪音的存在，学习过程会出现波动。因此，它在最小值的区域徘徊，不会收敛。

（2）学习过程会有更多的振荡，为更接近最小值，需要增加学习率衰减项，以降低学习率，避免过度振荡。

（3）批量大小的不当选择可能会带来一些问题。
